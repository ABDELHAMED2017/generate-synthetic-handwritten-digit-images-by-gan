{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8L58_F34yNmE"
   },
   "source": [
    "##IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1864,
     "status": "ok",
     "timestamp": 1551572996832,
     "user": {
      "displayName": "Cheng Ran",
      "photoUrl": "",
      "userId": "09094793715809051166"
     },
     "user_tz": 0
    },
    "id": "6kmqEx-tzOE9",
    "outputId": "0df68d44-615e-42b5-f1f2-6571a6d3f00c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 10814646953848160520, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 1975433551953419838\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d1EqFKey9OM"
   },
   "source": [
    "##Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4831,
     "status": "ok",
     "timestamp": 1551572999830,
     "user": {
      "displayName": "Cheng Ran",
      "photoUrl": "",
      "userId": "09094793715809051166"
     },
     "user_tz": 0
    },
    "id": "jD2fgbJezDKC",
    "outputId": "9c42d995-f022-4ce6-e7ee-823fba75b2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-566519a95339>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "\n",
      "Image Shape: (28, 28, 1)\n",
      "\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2QbcvHF0GGc"
   },
   "source": [
    "##Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5967,
     "status": "ok",
     "timestamp": 1551573000980,
     "user": {
      "displayName": "Cheng Ran",
      "photoUrl": "",
      "userId": "09094793715809051166"
     },
     "user_tz": 0
    },
    "id": "ZWNytqji0Hyi",
    "outputId": "4876798d-f0d1-473c-d6a9-0b57f103e384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (55000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Pad images with 0s\n",
    "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUM7X12i0WhU"
   },
   "outputs": [],
   "source": [
    "#Shuffle the training data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQGb8uT70xeJ"
   },
   "source": [
    "##Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOqqWJOF00ZB"
   },
   "outputs": [],
   "source": [
    "def generator(Z, initializer):\n",
    "    '''\n",
    "    Takes an argument Z, an [M, 1, 1, 100] tensor of random numbers.\n",
    "    Returns an operation that creates a generated image of shape [None, 32, 32, 1]\n",
    "    '''\n",
    "\n",
    "    with tf.variable_scope('generator'):\n",
    "\n",
    "        # Layer 1 -> [None, 4, 4, 512]\n",
    "        deconv_1 = tf.layers.conv2d_transpose(\n",
    "            Z, \n",
    "            filters=512, \n",
    "            kernel_size=[4,4], \n",
    "            strides=[1,1], \n",
    "            padding='valid',\n",
    "            kernel_initializer=initializer, \n",
    "            name='layer1')\n",
    "        norm_1 = tf.layers.batch_normalization(deconv_1)\n",
    "        lrelu_1 = tf.nn.leaky_relu(norm_1)\n",
    "\n",
    "        # Layer 2 -> [None, 8, 8, 256]\n",
    "        deconv_2 = tf.layers.conv2d_transpose(\n",
    "            lrelu_1, \n",
    "            filters=256, \n",
    "            kernel_size=[4,4], \n",
    "            strides=[2,2], \n",
    "            padding='same', \n",
    "            kernel_initializer=initializer,\n",
    "            name='layer2')\n",
    "        norm_2 = tf.layers.batch_normalization(deconv_2)\n",
    "        lrelu_2 = tf.nn.leaky_relu(norm_2)\n",
    "\n",
    "        # Layer 3 -> [None, 16, 16, 128]\n",
    "        deconv_3 = tf.layers.conv2d_transpose(\n",
    "            lrelu_2,\n",
    "            filters=128, \n",
    "            kernel_size=[4,4], \n",
    "            strides=[2,2], \n",
    "            padding='same', \n",
    "            kernel_initializer=initializer,\n",
    "            name='layer3')\n",
    "        norm_3 = tf.layers.batch_normalization(deconv_3)\n",
    "        lrelu_3 = tf.nn.leaky_relu(norm_3)\n",
    "\n",
    "        # Layer 4 -> [None, 32, 32, 1]\n",
    "        deconv_4 = tf.layers.conv2d_transpose(\n",
    "            lrelu_3, \n",
    "            filters=1,\n",
    "            kernel_size=[4,4],\n",
    "            strides=[2,2],\n",
    "            padding='same',\n",
    "            activation=tf.nn.tanh,\n",
    "            kernel_initializer=initializer,\n",
    "            name='layer4')\n",
    "        output = tf.identity(deconv_4, name='generated_images')\n",
    "\n",
    "        # Generated images of shape [M, 32, 32, 1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjJ9427A6ymQ"
   },
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWCfn7D16z2N"
   },
   "outputs": [],
   "source": [
    "def discriminator(images, initializer, reuse=False):\n",
    "    '''\n",
    "    Takes an image as an argument [None, 32, 32, 1].\n",
    "    Returns an operation that gives the probability of that image being 'real' [None, 1]\n",
    "    '''\n",
    "\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "\n",
    "        # Layer 1 -> [None, 16, 16, 128]\n",
    "        conv_1 = tf.layers.conv2d(\n",
    "            images, \n",
    "            filters=128, \n",
    "            kernel_size=[4,4], \n",
    "            strides=[2,2], \n",
    "            padding='same', \n",
    "            activation=tf.nn.leaky_relu,\n",
    "            kernel_initializer=initializer, \n",
    "            name='layer1')\n",
    "\n",
    "        # Layer 2 -> [None, 8, 8, 256]\n",
    "        conv_2 = tf.layers.conv2d(\n",
    "            conv_1, \n",
    "            filters=256, \n",
    "            kernel_size=[4,4], \n",
    "            strides=[2,2], \n",
    "            padding='same', \n",
    "            kernel_initializer=initializer,\n",
    "            name='layer2')\n",
    "        norm_2 = tf.layers.batch_normalization(conv_2)\n",
    "        lrelu_2 = tf.nn.leaky_relu(norm_2)\n",
    "\n",
    "        # Layer 3  -> [None, 4, 4, 512]\n",
    "        conv_3 = tf.layers.conv2d(\n",
    "            lrelu_2, \n",
    "            filters=512, \n",
    "            kernel_size=[4,4], \n",
    "            strides=[2,2], \n",
    "            padding='same', \n",
    "            kernel_initializer=initializer, \n",
    "            name='layer3')\n",
    "        norm_3 = tf.layers.batch_normalization(conv_3)\n",
    "        lrelu_3 = tf.nn.leaky_relu(norm_3)\n",
    "\n",
    "        # Layer 5 -> [None, 1, 1, 1]\n",
    "        conv_4 = tf.layers.conv2d(lrelu_3, filters=1, kernel_size=[4,4], strides=[1,1], padding='valid')\n",
    "        sigmoid_4 = tf.nn.sigmoid(conv_4)\n",
    "        output = tf.reshape(sigmoid_4, [-1, 1])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nun8cJR565Fx"
   },
   "source": [
    "##Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6g_krX7l68HB"
   },
   "outputs": [],
   "source": [
    "def loss(Dx, Dg):\n",
    "    '''\n",
    "    Dx = Probabilities assigned by D to the real images, [M, 1]\n",
    "    Dg = Probabilities assigned by D to the generated images, [M, 1]\n",
    "    '''\n",
    "    with tf.variable_scope('loss'):\n",
    "        loss_d = tf.identity(-tf.reduce_mean(tf.log(Dx) + tf.log(1. - Dg)), name='loss_d')\n",
    "        loss_g = tf.identity(-tf.reduce_mean(tf.log(Dg)), name='loss_g')\n",
    "        return loss_d, loss_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FC9-2_MFjLxB"
   },
   "source": [
    "##Train Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7149,
     "status": "ok",
     "timestamp": 1551573002219,
     "user": {
      "displayName": "Cheng Ran",
      "photoUrl": "",
      "userId": "09094793715809051166"
     },
     "user_tz": 0
    },
    "id": "6LkJec0cjP6h",
    "outputId": "4cc9a6bb-50ca-4e57-94c5-c49eecb54759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f1447741b12b>:17: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d_transpose instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-f1447741b12b>:18: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-8f50f19f6ed6>:18: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n"
     ]
    }
   ],
   "source": [
    "#set EPOCHS and BATCH for triaing\n",
    "EPOCHS = 30     #30 entries\n",
    "BATCH_SIZE = 256    #each entries have 128 samples\n",
    "#set Learning rate for G and N\n",
    "Rate_G = 0.0002\n",
    "Rate_D = 0.0002\n",
    "\n",
    "\n",
    "images_holder = tf.placeholder(tf.float32, shape=[None, 32, 32, 1], name='images_holder')\n",
    "Z_holder = tf.placeholder(tf.float32, shape=[None, 1, 1, 100], name='z_holder')\n",
    "\n",
    "# Forward\n",
    "weights_initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "generated_images = generator(Z_holder, weights_initializer)\n",
    "Dx = discriminator(images_holder, weights_initializer, False)\n",
    "Dg = discriminator(generated_images, weights_initializer, True)\n",
    "            \n",
    "# Loss： Dx:real; Dg:Fake\n",
    "loss_d, loss_g = loss(Dx, Dg)\n",
    "            \n",
    "# Optimize G or D\n",
    "optimizer_g = tf.train.AdamOptimizer(learning_rate=Rate_G, beta1=0.5)\n",
    "optimizer_d = tf.train.AdamOptimizer(learning_rate=Rate_D, beta1=0.5)\n",
    "            \n",
    "# Back propagation\n",
    "g_vars = tf.trainable_variables(scope='generator')\n",
    "d_vars = tf.trainable_variables(scope='discriminator')\n",
    "\n",
    "# Training \n",
    "train_G = optimizer_g.minimize(loss_g, var_list=g_vars, name='train_G')\n",
    "train_D = optimizer_d.minimize(loss_d, var_list = d_vars, name='train_D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10HTSWxR7seW"
   },
   "source": [
    "##Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6742,
     "status": "error",
     "timestamp": 1551555476848,
     "user": {
      "displayName": "Cheng Ran",
      "photoUrl": "",
      "userId": "09094793715809051166"
     },
     "user_tz": 0
    },
    "id": "cY4Ewwuj-EQh",
    "outputId": "8ff4bc7f-d39f-48b3-c3dd-5a673dc14bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Get data\n",
      "Training G\n",
      "Training D\n",
      "Get data\n",
      "Training G\n",
      "Training D\n",
      "Get data\n",
      "Training G\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    num_examples = len(X_train)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Training...\")\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        X_train = shuffle(X_train)\n",
    "        \n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            print(\"Get data\")\n",
    "            images = X_train[offset:end]\n",
    "            images = images / 128.\n",
    "            images = images - 1.\n",
    "            Z = np.random.normal(0.0, 1.0, size=[images.shape[0], 1, 1, 100])\n",
    "           # feed_dict = {'images_holder:0': batch_x 'z_holder:0': Z}\n",
    "            print(\"Training G\")\n",
    "            sess.run(train_G,feed_dict={'images_holder:0': images, 'z_holder:0': Z})\n",
    "            print(\"Training D\")\n",
    "            sess.run(train_D,feed_dict={'images_holder:0': images, 'z_holder:0': Z})\n",
    "            \n",
    "        if ((i+1)%10 == 0)or(i==0):\n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            #print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "            print()\n",
    "   \n",
    "\n",
    "print(\"END\")\n",
    "#plt.imshow((image[9, :, :, 0]+1)*128)\n",
    "#plt.imshow(X_train[9].squeeze())\n",
    "#https://github.com/sarahwolf32/DCGAN-for-MNIST/blob/master/trainer/task.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DCGAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
