{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CGAN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S9_vgfdxISgG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conditional DCGAN"
      ]
    },
    {
      "metadata": {
        "id": "5KmzubwgIovr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "nfen8HBmIXD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, time, random,itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "import pdb\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NpYy68hFIq8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ]
    },
    {
      "metadata": {
        "id": "6Kvar8EjI9Z4",
        "colab_type": "code",
        "outputId": "118f0d8e-9e84-452d-f260-8397ad2edadb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=[])\n",
        "X_train, y_train_cgan           = mnist.train.images, mnist.train.labels\n",
        "\n",
        "x_train = (X_train - 0.5) / 0.5  \n",
        "\n",
        "\n",
        "\n",
        "mnist_classifier = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "X_train, y_train           = mnist_classifier.train.images, mnist_classifier.train.labels\n",
        "X_validation, y_validation = mnist_classifier.validation.images, mnist_classifier.validation.labels\n",
        "X_test, y_test             = mnist_classifier.test.images, mnist_classifier.test.labels\n",
        "\n",
        " \n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "onehot = np.eye(10)\n",
        "\n",
        "def leaky_relu(X, leak=0.2):\n",
        "    f1 = 0.5 * (1 + leak)\n",
        "    f2 = 0.5 * (1 - leak)\n",
        "    return f1 * X + f2 * tf.abs(X)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-23de801d9e27>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vVXBp8bo7zPy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier"
      ]
    },
    {
      "metadata": {
        "id": "4_xa0Kys74RQ",
        "colab_type": "code",
        "outputId": "925799ce-da0b-4ad0-8b3a-670a972e1cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "#Classifier parameters\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "Rate_C = 0.005\n",
        "\n",
        "#Placeholder\n",
        "x_Classfier = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "y_Classfier = tf.placeholder(tf.int32, (None))\n",
        "one_hot_y_Classfier = tf.one_hot(y_Classfier, 10)\n",
        "\n",
        "#zero padding\n",
        "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "\n",
        "#classifier network\n",
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    \n",
        "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
        "    # Input: BATCH_SIZEx32x32x1\n",
        "    conv1_w = tf.Variable(tf.truncated_normal(shape = [5,5,1,6], mean = mu, stddev = sigma))\n",
        "    conv1_b = tf.Variable(tf.zeros(6))\n",
        "    conv1 = tf.nn.conv2d(x, conv1_w, strides = [1,1,1,1], padding = 'VALID') + conv1_b \n",
        "    \n",
        "    # Activation. max(0,conv1)\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
        "    # [Batch,Height,Width,Depth]\n",
        "    pool_1 = tf.nn.max_pool(conv1,ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
        "    \n",
        "    # Layer 2: Convolutional. Output = 10x10x16.\n",
        "    conv2_w = tf.Variable(tf.truncated_normal(shape = [5,5,6,16], mean = mu, stddev = sigma))\n",
        "    conv2_b = tf.Variable(tf.zeros(16))\n",
        "    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [1,1,1,1], padding = 'VALID') + conv2_b\n",
        "    \n",
        "    # Activation.\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
        "    pool_2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID') \n",
        "    \n",
        "    # Flatten. Input = 5x5x16. Output = 400.\n",
        "    fc1 = flatten(pool_2)\n",
        "    \n",
        "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fc1_w = tf.Variable(tf.truncated_normal(shape = (400,120), mean = mu, stddev = sigma))\n",
        "    fc1_b = tf.Variable(tf.zeros(120))\n",
        "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
        "    \n",
        "    # Activation.\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "    fc2_w = tf.Variable(tf.truncated_normal(shape = (120,84), mean = mu, stddev = sigma))\n",
        "    fc2_b = tf.Variable(tf.zeros(84))\n",
        "    fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
        "    \n",
        "    # Activation.\n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    \n",
        "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
        "    fc3_w = tf.Variable(tf.truncated_normal(shape = (84,10), mean = mu , stddev = sigma))\n",
        "    fc3_b = tf.Variable(tf.zeros(10))\n",
        "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
        "    return logits\n",
        "  \n",
        "# Loss function\n",
        "logits = LeNet(x_Classfier)\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y_Classfier, logits=logits)  \n",
        "loss_operation = tf.reduce_mean(cross_entropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=Rate_C)\n",
        "# AdagradOptimizer, or MomentumOptimizer\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=rate)\n",
        "training_operation = optimizer.minimize(loss_operation)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-397ab344d077>:74: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C-PuJS6j9S1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function to evaluate the model"
      ]
    },
    {
      "metadata": {
        "id": "rs8Dg-oZ9Yhi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions_operation = tf.argmax(tf.nn.softmax(logits), 1)\n",
        "correct_prediction = tf.equal(predictions_operation, tf.argmax(one_hot_y_Classfier, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    sess = tf.get_default_session()\n",
        "    all_predictions = []\n",
        "    # i = 0:BATCH_SIZE:num_examples\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "        accuracy, predictions = sess.run([accuracy_operation, predictions_operation], feed_dict={x_Classfier: batch_x, y_Classfier: batch_y})\n",
        "        \n",
        "        all_predictions = np.hstack((all_predictions, predictions))\n",
        "        total_accuracy += (accuracy * len(batch_x))\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    return total_accuracy / num_examples, all_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4bWwxBHw3KzA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Placeholder"
      ]
    },
    {
      "metadata": {
        "id": "6O1mDTe83NkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "noise = tf.placeholder(tf.float32, shape=(None, 1, 1, 100))\n",
        "labels = tf.placeholder(tf.float32, shape=(None, 1, 1, 10))\n",
        "real = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 10))\n",
        "Training = tf.placeholder(dtype=tf.bool)\n",
        "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_qMaCMk2JAoA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ]
    },
    {
      "metadata": {
        "id": "0VdTQKIZJFsG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Generator(x, labels,keep_prob=keep_prob, Training=True, reuse=False):\n",
        "    with tf.variable_scope('Generator', reuse=reuse):\n",
        "        W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        b = tf.constant_initializer(0.0)\n",
        "\n",
        "        concat = tf.concat([x, labels], 3)\n",
        "\n",
        "        out_1 = tf.layers.conv2d_transpose(concat, 256, [7, 7], strides=(1, 1), padding='valid', kernel_initializer=W, bias_initializer=b)\n",
        "        out_1 = tf.layers.dropout(out_1, keep_prob)\n",
        "        out_1 = tf.layers.batch_normalization(out_1, training=Training)#batch norm\n",
        "        out_1 = leaky_relu(out_1, 0.2)\n",
        "\n",
        "        out_2 = tf.layers.conv2d_transpose(out_1, 128, [5, 5], strides=(2, 2), padding='same', kernel_initializer=W, bias_initializer=b)\n",
        "        out_2 = tf.layers.dropout(out_2, keep_prob)\n",
        "        out_2 = tf.layers.batch_normalization(out_2, training=Training)#batch norm\n",
        "        out_2 = leaky_relu(out_2, 0.2)\n",
        "\n",
        "        out_3 = tf.layers.conv2d_transpose(out_2, 1, [5, 5], strides=(2, 2), padding='same', kernel_initializer=W, bias_initializer=b)\n",
        "        out_3 = tf.nn.tanh(out_3)\n",
        "        return out_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ajNg7X6JNRm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ]
    },
    {
      "metadata": {
        "id": "uPGatNMnJOqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Discriminator(x, real,keep_prob=keep_prob, Training=True, reuse=False):\n",
        "    with tf.variable_scope('Discriminator', reuse=reuse):\n",
        "\n",
        "        W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        b = tf.constant_initializer(0.0)\n",
        "\n",
        "        concat = tf.concat([x, real], 3)\n",
        "\n",
        "        out_1 = tf.layers.conv2d(concat, 128, [5, 5], strides=(2, 2), padding='same', kernel_initializer=W, bias_initializer=b)\n",
        "        out_1 = tf.layers.dropout(out_1, keep_prob)\n",
        "        out_1 = tf.layers.batch_normalization(out_1, training=Training)#batch norm\n",
        "        out_1 = leaky_relu(out_1, 0.2)\n",
        "\n",
        "        out_2 = tf.layers.conv2d(out_1, 256, [5, 5], strides=(2, 2), padding='same', kernel_initializer=W, bias_initializer=b)\n",
        "        out_2 = tf.layers.dropout(out_2, keep_prob)\n",
        "        out_2 = tf.layers.batch_normalization(out_2, training=Training)#batch norm\n",
        "        out_2 = leaky_relu(out_2, 0.2)\n",
        "\n",
        "        out_3 = tf.layers.conv2d(out_2, 1, [7, 7], strides=(1, 1), padding='valid', kernel_initializer=W, bias_initializer=b)\n",
        "        logits = tf.nn.sigmoid(out_3)\n",
        "        return logits, out_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QiTPVdsbKjVI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ]
    },
    {
      "metadata": {
        "id": "tn3RJESPKm19",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCH = 20\n",
        "BATCH_SIZE = 200\n",
        "keep_prob_train = 0.6\n",
        "BETA1 = 0.5\n",
        "lr = 0.0002\n",
        "label_smooth = 1  # to use smoothing, please modify the loss function block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TycJaszI0_QV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ]
    },
    {
      "metadata": {
        "id": "8b0WLH2nJ613",
        "colab_type": "code",
        "outputId": "9656e099-7458-4e00-ef0e-c819ea4202e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate images\n",
        "G_noise = Generator(noise, labels, Training)\n",
        "# D\n",
        "D_real, D_real_logits = Discriminator(x, real, Training)\n",
        "D_fake, D_fake_logits = Discriminator(G_noise, real, Training, reuse=True)\n",
        "\n",
        "# D real loss\n",
        "Dis_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.multiply(tf.ones_like(D_real_logits), (label_smooth))))\n",
        "# D generated image loss\n",
        "Dis_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([BATCH_SIZE, 1, 1, 1])))\n",
        "# D total loss\n",
        "Dis_loss = (Dis_loss_real + Dis_loss_fake) \n",
        "# G loss\n",
        "Gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([BATCH_SIZE, 1, 1, 1])))\n",
        "\n",
        "# get all variables\n",
        "tf_vars = tf.trainable_variables()\n",
        "Dis_vars = [var for var in tf_vars if var.name.startswith('Discriminator')]\n",
        "Gen_vars = [var for var in tf_vars if var.name.startswith('Generator')]\n",
        "# optimise\n",
        "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "    D_optim =tf.train.AdamOptimizer(lr, beta1=BETA1).minimize(Dis_loss, var_list=Dis_vars)\n",
        "    G_optim = tf.train.AdamOptimizer(lr, beta1=BETA1).minimize(Gen_loss, var_list=Gen_vars)\n",
        "    \n",
        "    #D_optim = tf.train.AdagradOptimizer(lr).minimize(Dis_loss, var_list=Dis_vars)\n",
        "    #G_optim = tf.train.AdagradOptimizer(lr).minimize(Gen_loss, var_list=Gen_vars)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-9689b1585b3f>:8: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d_transpose instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-9689b1585b3f>:9: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-9689b1585b3f>:10: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-7-6314a7bdda3e>:9: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "POC9cA26KMoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "D2rrQ0mLKQ_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "8a1f9e94-2c68-4a6c-ca64-9a41563de267"
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "\n",
        "n_sample = 10000\n",
        "y_generated = np.zeros(shape=[10*n_sample,1,1,10])\n",
        "y_ = np.zeros(shape=[10*n_sample])\n",
        "\n",
        "for i in range(10):   \n",
        "    y_generated[n_sample*i:n_sample*(i+1),:,:, i] = 1\n",
        "    y_[n_sample*i:n_sample*(i+1)] = i\n",
        "    \n",
        "num_examples = len(x_train) \n",
        "k = num_examples % BATCH_SIZE\n",
        "num_examples_gan = num_examples - k\n",
        "\n",
        "G_loss = []\n",
        "D_loss = []\n",
        "\n",
        "D_r = []\n",
        "D_f = []\n",
        "\n",
        "Acc_DCGAN = np.zeros(shape=[EPOCH,10])\n",
        "Acc_test = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    print(\"Training Classifier\")  \n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "         \n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "            sess.run(training_operation, feed_dict={x_Classfier: batch_x, y_Classfier: batch_y})\n",
        "            \n",
        "        validation_accuracy, _ = evaluate(X_validation, y_validation)\n",
        "        if ((i+1)%10 == 0)or(i==0):\n",
        "            print(\"EPOCH {} ...\".format(i+1))\n",
        "            print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
        "            print()\n",
        "            \n",
        "            \n",
        "    print(\"Training CDCGAN\")    \n",
        "    for i in range(EPOCH):  \n",
        "        start = time.time()        \n",
        "        x_train, y_train_cgan = shuffle(x_train, y_train_cgan)    \n",
        "        for offset in range(0, num_examples_gan, BATCH_SIZE):\n",
        "            train_d = True\n",
        "            train_g = True                        \n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x = x_train[offset:end] \n",
        "            batch_y = y_train_cgan[offset:end] \n",
        "            \n",
        "            label_ = batch_y.reshape([BATCH_SIZE, 1, 1, 10])\n",
        "            real_ = label_ * np.ones([BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 10])\n",
        "            noise_ = np.random.normal(0, 1, (BATCH_SIZE, 1, 1, 100))\n",
        "                        \n",
        "            #calculate loss\n",
        "            d_ls = sess.run(Dis_loss,{noise: noise_, x: batch_x, real: real_, labels: label_, Training: False})\n",
        "            g_ls = sess.run(Gen_loss,{noise: noise_, x: batch_x, real: real_, labels: label_, Training: False})\n",
        "                                    \n",
        "            #Gobal loss          \n",
        "            d_r = sess.run([D_real], {x: batch_x, real: real_, labels: label_, Training: False})  \n",
        "            d_f = sess.run([D_fake], {noise: noise_, real: real_, labels: label_, Training: False}) \n",
        "            \n",
        "            d_r = np.mean(d_r)\n",
        "            d_f = np.mean(d_f)\n",
        "            #break\n",
        "            D_r.append(d_r)\n",
        "            D_f.append(d_f)\n",
        "                  \n",
        "            D_loss.append(d_ls)\n",
        "            G_loss.append(g_ls)\n",
        "            \n",
        "            if g_ls * 2 < d_ls:\n",
        "                train_g = False\n",
        "                pass\n",
        "            if d_ls * 2 < g_ls:\n",
        "                train_d = False\n",
        "                pass\n",
        "            \n",
        "            #Update D \n",
        "            if train_d:\n",
        "                sess.run(D_optim, {x: batch_x, noise: noise_,keep_prob: keep_prob_train,real: real_, labels: label_,Training: True})\n",
        "            \n",
        "            #Update G\n",
        "            if train_g:\n",
        "                sess.run(G_optim, {noise: noise_, x:batch_x,keep_prob: keep_prob_train,real: real_, labels: label_, Training: True})\n",
        "                \n",
        "            \n",
        "            \n",
        "        # Calculate accuracy\n",
        "        for k in range(0,10):\n",
        "            n =  np.random.normal(0, 1, (n_sample, 1, 1, 100))\n",
        "            samples = sess.run(G_noise, feed_dict={noise:n, labels:y_generated[n_sample*k:n_sample*(k+1),:,:,:], Training:False})   \n",
        "            samples = samples*2 + 0.5\n",
        "            x_ = np.pad(samples, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "        \n",
        "            generated_accuracy, _ = evaluate(x_, y_[n_sample*k:n_sample*(k+1)])\n",
        "            Acc_DCGAN[i,k] = generated_accuracy\n",
        "            \n",
        "        test_accuracy, _ = evaluate(X_test, y_test)\n",
        "        Acc_test.append(test_accuracy)       \n",
        "        end = time.time()\n",
        "        elapsed = end - start    \n",
        "        #break\n",
        "        if ((i+1)%2 == 0)or(i==0):\n",
        "            print(\"EPOCH {} ...\".format(i+1))\n",
        "            print(\"G_loss = {:.3f}  D_loss = {:.3f}  Average accuracy = {:.3f}  Tme used = {:.3f}\".format(g_ls, d_ls,np.mean(Acc_DCGAN[i,:]),elapsed))\n",
        "            print()     \n",
        "\n",
        "            \n",
        "    saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 0.979\n",
            "\n",
            "EPOCH 10 ...\n",
            "Validation Accuracy = 0.989\n",
            "\n",
            "EPOCH 20 ...\n",
            "Validation Accuracy = 0.987\n",
            "\n",
            "Training CDCGAN\n",
            "EPOCH 1 ...\n",
            "G_loss = 0.759  D_loss = 1.001  Average accuracy = 0.281  Tme used = 151.606\n",
            "\n",
            "EPOCH 2 ...\n",
            "G_loss = 1.390  D_loss = 0.729  Average accuracy = 0.399  Tme used = 147.164\n",
            "\n",
            "EPOCH 4 ...\n",
            "G_loss = 2.218  D_loss = 1.177  Average accuracy = 0.518  Tme used = 138.951\n",
            "\n",
            "EPOCH 6 ...\n",
            "G_loss = 1.134  D_loss = 1.042  Average accuracy = 0.630  Tme used = 154.627\n",
            "\n",
            "EPOCH 8 ...\n",
            "G_loss = 1.011  D_loss = 1.037  Average accuracy = 0.666  Tme used = 154.296\n",
            "\n",
            "EPOCH 10 ...\n",
            "G_loss = 1.322  D_loss = 1.086  Average accuracy = 0.645  Tme used = 154.742\n",
            "\n",
            "EPOCH 12 ...\n",
            "G_loss = 1.298  D_loss = 1.059  Average accuracy = 0.688  Tme used = 153.792\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "svoByV1F-OZK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inception score"
      ]
    },
    {
      "metadata": {
        "id": "7WTCFYGI-O-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Ave_acc = np.zeros(shape=[EPOCH])\n",
        "for i in range(0,EPOCH):\n",
        "    Ave_acc[i] = np.mean(Acc_DCGAN[i,:])\n",
        "print(\"Average accuracy:\")   \n",
        "print(Ave_acc)\n",
        "    \n",
        "for k in range(0,10):\n",
        "    print(\"Accuracy of class: {}\". format(k))   \n",
        "    print(Acc_DCGAN[:,k])\n",
        "\n",
        "N = len(Acc_test)\n",
        "index = np.arange(1,N+1,1)\n",
        "\n",
        "f_d = plt.figure(1)\n",
        "for k in range(0,10):\n",
        "    plt.plot(index, Acc_DCGAN[:,k],'--',linewidth=3,label='Class:'+str(k))\n",
        "        \n",
        "plt.plot(index, Acc_test, 'b',linewidth=3, label='Testing Set')\n",
        "plt.plot(index,Ave_acc, 'r', linewidth=3, label='Average')\n",
        "plt.xlabel(\"EPOCH\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.show()\n",
        "\n",
        "f_d.savefig('Accuracy.png', dpi=600)\n",
        "files.download('Accuracy.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9boaRjia6EDt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## D real and fake loss"
      ]
    },
    {
      "metadata": {
        "id": "AMwoncA46EqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D_r_mean = []\n",
        "D_f_mean = []\n",
        "\n",
        "N = len(D_r)\n",
        "length = N // (EPOCH)\n",
        "\n",
        "for k in range(0,EPOCH):\n",
        "  D_r_mean.append( np.mean(D_r[(k+1)*length -10 : (k+1)*length + 10] ))\n",
        "  D_f_mean.append( np.mean(D_f[(k+1)*length -10 : (k+1)*length + 10] ))\n",
        "\n",
        "print(\"Average D real loss\")  \n",
        "print(D_r_mean)\n",
        "print(\"Average D fake loss\")\n",
        "print(D_f_mean)\n",
        "\n",
        "index = np.arange(1,EPOCH+1,1)\n",
        "f_d = plt.figure(1)\n",
        "plt.plot(index, D_r_mean, 'r',label='D Real')\n",
        "plt.plot(index, D_f_mean, 'b',label='D Fake')\n",
        "plt.ylabel(\"D Loss\")\n",
        "plt.xlabel(\"EPOCH\")\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "f_d.savefig('Real and fake Loss.png', dpi=600)\n",
        "\n",
        "files.download('Real and fake Loss.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2j4rAJOm6RAc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot loss"
      ]
    },
    {
      "metadata": {
        "id": "e3TjGFfl6R3d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d_s_mean = []\n",
        "g_s_mean = []\n",
        "\n",
        "N = len(D_loss)\n",
        "length = N // (EPOCH)\n",
        "\n",
        "for k in range(0,EPOCH):\n",
        "  d_s_mean.append( np.mean(D_loss[(k+1)*length -10 : (k+1)*length + 10] ))\n",
        "  g_s_mean.append( np.mean(G_loss[(k+1)*length -10 : (k+1)*length + 10] ))\n",
        "\n",
        "print(\"Average D loss\")  \n",
        "print(d_s_mean)\n",
        "print(\"Average G loss\")\n",
        "print(g_s_mean)\n",
        "\n",
        "index = np.arange(1,EPOCH+1,1)\n",
        "f = plt.figure(1)\n",
        "plt.plot(index, d_s_mean, 'r',label='D Loss')\n",
        "plt.plot(index, g_s_mean, 'b',label='G Loss')\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"EPOCH\")\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.show()\n",
        "\n",
        "f.savefig('Loss.png', dpi=600)\n",
        "\n",
        "files.download('Loss.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRAQaJPmLPKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ]
    },
    {
      "metadata": {
        "id": "OfSdVjgBNg_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_result(test_images, show = True, save = True):\n",
        "    size_figure_grid = 10\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "    for k in range(10*10):\n",
        "        i = k // 10\n",
        "        j = k % 10\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (IMAGE_SIZE, IMAGE_SIZE)), cmap='gray')\n",
        "        \n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "    if save:    \n",
        "        fig.savefig('G images.png', dpi=600)\n",
        "        files.download('G images.png')\n",
        "        \n",
        "        \n",
        "noise_ = np.random.normal(0, 1, (100, 1, 1, 100))\n",
        "fixed_label_ = np.zeros((10, 1))\n",
        "\n",
        "for i in range(9):\n",
        "    temp = np.ones((10, 1)) + i\n",
        "    fixed_label_ = np.concatenate([fixed_label_, temp], 0)\n",
        "fixed_label_ = onehot[fixed_label_.astype(np.int32)].reshape((100, 1, 1, 10))\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('.'))    \n",
        "    test_images = sess.run(G_noise, {noise: noise_, labels: fixed_label_, Training: False})\n",
        "\n",
        "\n",
        "show_result(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}